{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10cc2be1",
   "metadata": {},
   "source": [
    "# Grad-CAM Visualization for Swin-Transformer\n",
    "\n",
    "This notebook demonstrates how to compute a Grad-CAM heatmap for a SwinTransformerClassifier. We will:\n",
    "\n",
    "1. Load necessary libraries and modules.\n",
    "2. Define hook functions for capturing activations and gradients.\n",
    "3. Preprocess an input image (resize, normalize) to feed into the model.\n",
    "4. Compute the Grad-CAM heatmap by backpropagating the target class score.\n",
    "5. Overlay the heatmap on the original image and display the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16aee451-3ddf-4ea3-a99a-cce6f0cc5f7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/realmmlab/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import cv2  # used for applying the colormap\n",
    "\n",
    "# Import the classifier\n",
    "from models.swin_transformer_classifier import SwinTransformerClassifier\n",
    "\n",
    "# Global containers to store the activation and its gradient\n",
    "activations = None\n",
    "gradients = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc33ff07",
   "metadata": {},
   "source": [
    "## Define Hook Functions\n",
    "\n",
    "These hook functions are used to capture intermediate activations and gradients from the target layer (which is chosen as the patch-embedding projection layer in the Swin Transformer). The forward hook stores the activations, and the backward hook stores the gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a7b542c-60f8-4616-9ee1-08e51a6650cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_hook(module, input, output):\n",
    "    global activations\n",
    "    if isinstance(output, tuple):\n",
    "        activations = output[0]  # Extract the first element if output is a tuple\n",
    "    else:\n",
    "        activations = output\n",
    "\n",
    "def backward_hook(module, grad_input, grad_output):\n",
    "    global gradients\n",
    "    if isinstance(grad_output, tuple):\n",
    "        gradients = grad_output[0]  # Extract the first element if grad_output is a tuple\n",
    "    else:\n",
    "        gradients = grad_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112a8f16",
   "metadata": {},
   "source": [
    "## Preprocess Image and Generate Heatmap Overlay\n",
    "\n",
    "The following function loads and preprocesses an image. It resizes the image to the target size, converts it into a tensor, and normalizes it with the same ImageNet means and standard deviations. We also keep a copy of the original image (as a PIL image) for the purpose of overlaying the generated heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94aee171-89d8-4e43-a3b1-20bc51b1a3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image_path, target_size=(384, 384)):\n",
    "    \"\"\"\n",
    "    Load and preprocess an image.\n",
    "    The processing steps include:\n",
    "      - Resize the image to the target size\n",
    "      - Convert the image to a tensor\n",
    "      - Normalize with ImageNet mean and std\n",
    "    \"\"\"\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(target_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    return transform(image).unsqueeze(0), image  # return tensor and original PIL image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49b25a0",
   "metadata": {},
   "source": [
    "Next, we define a function to generate a heatmap overlay on the original image. This function applies a color map to the Grad-CAM output and then overlays it on the input image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8ee003-aae0-4b4f-88e2-0f30a8c9f6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_heatmap_on_image(original_img, cam, alpha=0.5):\n",
    "    \"\"\"\n",
    "    Overlay the heatmap on the original image.\n",
    "    Args:\n",
    "        original_img: PIL Image\n",
    "        cam: numpy array (H, W), normalized between 0 and 1.\n",
    "        alpha: Transparency of the heatmap.\n",
    "    Returns:\n",
    "        overlay: Heatmap overlaid on the original image (numpy array).\n",
    "        heatmap: Heatmap only (numpy array).\n",
    "    \"\"\"\n",
    "    img = np.array(original_img)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "    cam = cv2.resize(cam, (img.shape[1], img.shape[0]))\n",
    "    heatmap = cv2.applyColorMap(np.uint8(255 * cam), cv2.COLORMAP_JET)\n",
    "    overlay = cv2.addWeighted(heatmap, alpha, img, 1 - alpha, 0)\n",
    "    overlay = cv2.cvtColor(overlay, cv2.COLOR_BGR2RGB)\n",
    "    heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)\n",
    "    return overlay, heatmap\n",
    "\n",
    "def visualize_heatmap(image_path, cam, class_name, alpha=0.5, save_path=None):\n",
    "    \"\"\"\n",
    "    Visualize the original image, overlay, heatmap, and colorbar in a single figure.\n",
    "    Args:\n",
    "        image_path: Path to the image file.\n",
    "        cam: numpy array (H, W), normalized between 0 and 1.\n",
    "        class_name: Predicted class name.\n",
    "        alpha: Transparency of the heatmap.\n",
    "        save_path: Optional path to save the visualization.\n",
    "    \"\"\"\n",
    "    original_img = Image.open(image_path)\n",
    "    overlay, heatmap = generate_heatmap_on_image(original_img, cam, alpha)\n",
    "    \n",
    "    # Extract the predicted class index from the heatmap (if provided)\n",
    "    predicted_class = np.argmax(cam) if cam.ndim == 2 else np.argmax(cam.sum(axis=(1, 2)))\n",
    "    predicted_class_name = class_names[predicted_class]\n",
    "    \n",
    "    # Extract the image name from the image_path\n",
    "    image_name = image_path.split(\"/\")[-1]\n",
    "    \n",
    "    # Create a figure with four subplots\n",
    "    fig = plt.figure(figsize=(16, 6))\n",
    "    gs = fig.add_gridspec(1, 4, width_ratios=[1, 1, 1, 0.05])  # Make colorbar narrower\n",
    "    \n",
    "    # Plot the original image\n",
    "    ax1 = fig.add_subplot(gs[0])\n",
    "    ax1.imshow(original_img)\n",
    "    ax1.axis('off')\n",
    "    ax1.set_title(f\"Original Image\\n{image_name}\\nClass: {predicted_class_name}\")\n",
    "    \n",
    "    # Plot the overlay\n",
    "    ax2 = fig.add_subplot(gs[1])\n",
    "    ax2.imshow(overlay)\n",
    "    ax2.axis('off')\n",
    "    ax2.set_title(f\"Overlay (Heatmap + Original)\\nClass: {predicted_class_name}\")\n",
    "    \n",
    "    # Plot the heatmap\n",
    "    ax3 = fig.add_subplot(gs[2])\n",
    "    ax3.imshow(heatmap)\n",
    "    ax3.axis('off')\n",
    "    ax3.set_title(f\"Heatmap Only\\nClass: {predicted_class_name}\")\n",
    "    \n",
    "    # Plot the colorbar\n",
    "    ax4 = fig.add_subplot(gs[3])\n",
    "    im = ax4.imshow(cam, cmap='jet', aspect='auto', vmin=0, vmax=1)\n",
    "    \n",
    "    # Add a smaller colorbar\n",
    "    cbar = plt.colorbar(im, cax=ax4, orientation='vertical', shrink=0.6)  # Shrink the colorbar\n",
    "    ax4.axis('off')\n",
    "    \n",
    "    # Add text below the colorbar\n",
    "    fig.text(0.88, 0.1, \"Heatmap Intensity\\n(Blue = Low, Red = High)\", \n",
    "             ha='center', va='center', fontsize=10, color='black')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, bbox_inches='tight', pad_inches=0.1)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e7a848",
   "metadata": {},
   "source": [
    "## Compute Grad-CAM\n",
    "\n",
    "In the function below we perform the following steps:\n",
    "\n",
    "1. Register forward and backward hooks to capture activations and gradients from the target layer. Here, we assume that the target layer is the patch-embedding projection layer (`model.swin_model.swin_model.patch_embed.proj`).\n",
    "2. Run a forward pass on the input image.\n",
    "3. Determine the target class (if not provided, the highest logit is chosen).\n",
    "4. Run a backward pass to compute gradients with respect to the target class score.\n",
    "5. Compute the Grad-CAM by pooling the gradients across the spatial dimensions and performing a weighted sum of the activations.\n",
    "6. Normalize the resulting heatmap between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "85ae1061-4e5d-4257-bd32-d8f5760ecc50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradcam(model, input_tensor, target_size=None, target_class=None):\n",
    "    global activations, gradients\n",
    "    activations = None\n",
    "    gradients = None\n",
    "\n",
    "    # Register hooks on the patch embeddings layer\n",
    "    target_layer = model.swin_model.swin.embeddings.patch_embeddings\n",
    "\n",
    "    hook_handle_forward = target_layer.register_forward_hook(forward_hook)\n",
    "    hook_handle_backward = target_layer.register_full_backward_hook(backward_hook)\n",
    "\n",
    "    model.zero_grad()  # Clear gradients before forward pass\n",
    "    input_tensor.requires_grad_(True)  # Ensure input tensor requires gradients\n",
    "\n",
    "    output = model(input_tensor)  # Forward pass\n",
    "    \n",
    "    if target_class is None:\n",
    "        target_class = output.argmax(dim=-1).item()\n",
    "\n",
    "    score = output[0, target_class]\n",
    "\n",
    "    score.backward()  # Backward pass\n",
    "\n",
    "    hook_handle_forward.remove()\n",
    "    hook_handle_backward.remove()\n",
    "\n",
    "    # Debugging: Check if gradients are captured\n",
    "    if gradients is None:\n",
    "        raise ValueError(\"Gradients were not captured. Check the backward hook and target layer.\")\n",
    "\n",
    "    # Reshape activations and gradients to [batch_size, channels, height, width]\n",
    "    if target_size is None:    \n",
    "        H, W = 384, 384  # Input image size\n",
    "    else:\n",
    "        (H, W) = target_size\n",
    "    P = 4  # Patch size\n",
    "    activations = activations.reshape(1, H // P, W // P, -1).permute(0, 3, 1, 2)\n",
    "    gradients = gradients.reshape(1, H // P, W // P, -1).permute(0, 3, 1, 2)\n",
    "\n",
    "    # Compute Grad-CAM\n",
    "    pooled_gradients = torch.mean(gradients, dim=[2, 3], keepdim=True)\n",
    "    weighted_activation = pooled_gradients * activations\n",
    "    cam = torch.sum(weighted_activation, dim=1).squeeze()\n",
    "    cam = torch.relu(cam)\n",
    "\n",
    "    cam_np = cam.detach().cpu().numpy()\n",
    "    if np.max(cam_np) != 0:\n",
    "        cam_np = cam_np / np.max(cam_np)\n",
    "    else:\n",
    "        cam_np = np.zeros_like(cam_np)\n",
    "\n",
    "    return cam_np, target_class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea38fd36",
   "metadata": {},
   "source": [
    "## Main Function\n",
    "\n",
    "The main function puts everything together:\n",
    "\n",
    "1. It sets configuration values like the image path, number of classes, and model weights path.\n",
    "2. It loads the pre-trained model using `SwinTransformerClassifier.load_model`.\n",
    "3. It pre-processes the input image and sends it to the proper device.\n",
    "4. It computes the Grad-CAM, prints the predicted class, and generates the heatmap overlay.\n",
    "5. Finally, it displays the result using matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1002bde-76cb-4df2-a1e3-a071bd4db45c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class:  0\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 56\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;66;03m# ----------------------------------\u001b[39;00m\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;66;03m# Visualize and save the result\u001b[39;00m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;66;03m# ----------------------------------\u001b[39;00m\n\u001b[1;32m     54\u001b[0m     visualize_heatmap(image_path, cam_np, class_names, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, save_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgradcam_overlay.png\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 56\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[25], line 54\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredicted class: \u001b[39m\u001b[38;5;124m\"\u001b[39m, predicted_class)\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# ----------------------------------\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# Visualize and save the result\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# ----------------------------------\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m \u001b[43mvisualize_heatmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcam_np\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgradcam_overlay.png\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[23], line 36\u001b[0m, in \u001b[0;36mvisualize_heatmap\u001b[0;34m(image_path, cam, class_names, alpha, save_path)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Extract the predicted class index from the heatmap (if provided)\u001b[39;00m\n\u001b[1;32m     35\u001b[0m predicted_class \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(cam) \u001b[38;5;28;01mif\u001b[39;00m cam\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m np\u001b[38;5;241m.\u001b[39margmax(cam\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)))\n\u001b[0;32m---> 36\u001b[0m predicted_class_name \u001b[38;5;241m=\u001b[39m \u001b[43mclass_names\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpredicted_class\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Extract the image name from the image_path\u001b[39;00m\n\u001b[1;32m     39\u001b[0m image_name \u001b[38;5;241m=\u001b[39m image_path\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # ----------------------------\n",
    "    # Configuration / arguments\n",
    "    # ----------------------------\n",
    "    # Path to an image file (change to your image file path)\n",
    "    image_path = \"split/classification/F/test/Gl6/22_676_P1_3FG_F.png\"\n",
    "    \n",
    "    # Set the number of classes in your model\n",
    "    num_classes = 3\n",
    "    target_size = (384, 384)\n",
    "    \n",
    "    # Class names\n",
    "    class_names = [\"Gl6\", \"Gl9\", \"Healthy\"]\n",
    "    \n",
    "    # Path to your model weights\n",
    "    model_weight_path = \"Model_F_swintransformerclassifier_final_20250221_153017.pth\"\n",
    "    \n",
    "    # Load the model using the provided weights\n",
    "    model = SwinTransformerClassifier.load_model(\n",
    "                model_weight_path=model_weight_path,\n",
    "                num_classes=num_classes,\n",
    "                train_path=None,\n",
    "                val_path=None,\n",
    "                test_path=None,\n",
    "                optimizer=\"adam\",\n",
    "                lr=1e-3,\n",
    "                batch_size=16,\n",
    "                transfer=True,\n",
    "                tune_fc_only=True,\n",
    "                target_size=target_size\n",
    "            )\n",
    "    model.eval()\n",
    "\n",
    "    # Put model on device (CPU or GPU)\n",
    "    # device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    device = torch.device(\"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    # -------------------------------\n",
    "    # Preprocess the image\n",
    "    # -------------------------------\n",
    "    input_tensor, original_img = preprocess_image(image_path, target_size=target_size)\n",
    "    input_tensor = input_tensor.to(device)\n",
    "\n",
    "    # ----------------------------------\n",
    "    # Compute Grad-CAM\n",
    "    # ----------------------------------\n",
    "    cam_np, predicted_class = compute_gradcam(model, input_tensor, target_size)\n",
    "    class_name = class_names[predicted_class]\n",
    "    print(\"Predicted class: \", class_name)\n",
    "\n",
    "    # ----------------------------------\n",
    "    # Visualize and save the result\n",
    "    # ----------------------------------\n",
    "    visualize_heatmap(image_path, cam_np, class_name, alpha=0.2, save_path=\"gradcam_overlay.png\")\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44d2c1a",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook showed how to:\n",
    "\n",
    "- Use forward and backward hooks in PyTorch to capture intermediate activations and gradients.\n",
    "- Preprocess an input image using standard transforms.\n",
    "- Compute a Grad-CAM heatmap highlighting the regions in the image that contributed most to the prediction.\n",
    "- Overlay the heatmap on the original image and visualize the final result.\n",
    "\n",
    "Make sure to update the file paths (`image_path` and `model_weight_path`) to point to your own image and model weight files before running the notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "realmmlab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
