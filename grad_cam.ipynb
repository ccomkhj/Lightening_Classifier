{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grad-CAM Visualization for Swin-Transformer\n",
    "\n",
    "This notebook demonstrates how to compute a Grad-CAM heatmap for a SwinTransformerClassifier. We will:\n",
    "\n",
    "1. Load necessary libraries and modules.\n",
    "2. Define hook functions for capturing activations and gradients.\n",
    "3. Preprocess an input image (resize, normalize) to feed into the model.\n",
    "4. Compute the Grad-CAM heatmap by backpropagating the target class score.\n",
    "5. Overlay the heatmap on the original image and display the result.\n",
    "\n",
    "Before running the notebook, ensure you have installed the following packages:\n",
    "\n",
    "- torch\n",
    "- torchvision\n",
    "- transformers (if needed by your model)\n",
    "- opencv-python\n",
    "- matplotlib\n",
    "- pillow\n",
    "\n",
    "You can install them via pip (for example, `pip install torch torchvision opencv-python matplotlib pillow`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16aee451-3ddf-4ea3-a99a-cce6f0cc5f7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/realmmlab/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import cv2  # used for applying the colormap\n",
    "\n",
    "# Import the classifier\n",
    "from models.swin_transformer_classifier import SwinTransformerClassifier\n",
    "\n",
    "# Global containers to store the activation and its gradient\n",
    "activations = None\n",
    "gradients = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Hook Functions\n",
    "\n",
    "These hook functions are used to capture intermediate activations and gradients from the target layer (which is chosen as the patch-embedding projection layer in the Swin Transformer). The forward hook stores the activations, and the backward hook stores the gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a7b542c-60f8-4616-9ee1-08e51a6650cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_hook(module, input, output):\n",
    "    global activations\n",
    "    if isinstance(output, tuple):\n",
    "        activations = output[0]  # Extract the first element if output is a tuple\n",
    "    else:\n",
    "        activations = output\n",
    "    print(\"Forward hook - Activations shape:\", activations.shape)  # Debugging: Print activations shape\n",
    "\n",
    "def backward_hook(module, grad_input, grad_output):\n",
    "    global gradients\n",
    "    if isinstance(grad_output, tuple):\n",
    "        gradients = grad_output[0]  # Extract the first element if grad_output is a tuple\n",
    "    else:\n",
    "        gradients = grad_output\n",
    "    print(\"Backward hook - Grad output shape:\", gradients.shape if gradients is not None else None)  # Debugging: Print grad_output shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Image and Generate Heatmap Overlay\n",
    "\n",
    "The following function loads and preprocesses an image. It resizes the image to the target size, converts it into a tensor, and normalizes it with the same ImageNet means and standard deviations. We also keep a copy of the original image (as a PIL image) for the purpose of overlaying the generated heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94aee171-89d8-4e43-a3b1-20bc51b1a3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image_path, target_size=(384, 384)):\n",
    "    \"\"\"\n",
    "    Load and preprocess an image.\n",
    "    The processing steps include:\n",
    "      - Resize the image to the target size\n",
    "      - Convert the image to a tensor\n",
    "      - Normalize with ImageNet mean and std\n",
    "    \"\"\"\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(target_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    return transform(image).unsqueeze(0), image  # return tensor and original PIL image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define a function to generate a heatmap overlay on the original image. This function uses OpenCV to apply a color map to the Grad-CAM output and then overlays it on the input image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd8ee003-aae0-4b4f-88e2-0f30a8c9f6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_heatmap_on_image(original_img, cam, alpha=0.5):\n",
    "    \"\"\"\n",
    "    Overlay the heatmap on the original image.\n",
    "      original_img: PIL Image\n",
    "      cam: numpy array (H, W), normalized between 0 and 1.\n",
    "    \"\"\"\n",
    "    # Convert original image into numpy array (RGB) and then to BGR for cv2\n",
    "    img = np.array(original_img)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "    \n",
    "    # Resize cam to match the image size\n",
    "    cam = cv2.resize(cam, (img.shape[1], img.shape[0]))\n",
    "    \n",
    "    # Convert the cam to a heatmap using cv2's COLORMAP_JET\n",
    "    heatmap = cv2.applyColorMap(np.uint8(255 * cam), cv2.COLORMAP_JET)\n",
    "    \n",
    "    # Combine heatmap with the original image\n",
    "    overlay = cv2.addWeighted(heatmap, alpha, img, 1 - alpha, 0)\n",
    "    \n",
    "    # Convert overlay back to RGB for displaying with matplotlib\n",
    "    overlay = cv2.cvtColor(overlay, cv2.COLOR_BGR2RGB)\n",
    "    return overlay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Grad-CAM\n",
    "\n",
    "In the function below we perform the following steps:\n",
    "\n",
    "1. Register forward and backward hooks to capture activations and gradients from the target layer. Here, we assume that the target layer is the patch-embedding projection layer (`model.swin_model.swin_model.patch_embed.proj`).\n",
    "2. Run a forward pass on the input image.\n",
    "3. Determine the target class (if not provided, the highest logit is chosen).\n",
    "4. Run a backward pass to compute gradients with respect to the target class score.\n",
    "5. Compute the Grad-CAM by pooling the gradients across the spatial dimensions and performing a weighted sum of the activations.\n",
    "6. Normalize the resulting heatmap between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ae1061-4e5d-4257-bd32-d8f5760ecc50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradcam(model, input_tensor, target_size=None, target_class=None):\n",
    "    global activations, gradients\n",
    "    activations = None\n",
    "    gradients = None\n",
    "\n",
    "    # Register hooks on the patch embeddings layer\n",
    "    target_layer = model.swin_model.swin.embeddings.patch_embeddings\n",
    "\n",
    "    hook_handle_forward = target_layer.register_forward_hook(forward_hook)\n",
    "    hook_handle_backward = target_layer.register_full_backward_hook(backward_hook)\n",
    "\n",
    "    model.zero_grad()  # Clear gradients before forward pass\n",
    "    input_tensor.requires_grad_(True)  # Ensure input tensor requires gradients\n",
    "\n",
    "    output = model(input_tensor)  # Forward pass\n",
    "    \n",
    "    if target_class is None:\n",
    "        target_class = output.argmax(dim=-1).item()\n",
    "\n",
    "    score = output[0, target_class]\n",
    "\n",
    "    score.backward()  # Backward pass\n",
    "\n",
    "    hook_handle_forward.remove()\n",
    "    hook_handle_backward.remove()\n",
    "\n",
    "    # Debugging: Check if gradients are captured\n",
    "    if gradients is None:\n",
    "        raise ValueError(\"Gradients were not captured. Check the backward hook and target layer.\")\n",
    "\n",
    "    # Reshape activations and gradients to [batch_size, channels, height, width]\n",
    "    if target_size is None:    \n",
    "        H, W = 384, 384  # Input image size\n",
    "    else:\n",
    "        (H, W) = target_size\n",
    "    P = 4  # Patch size\n",
    "    activations = activations.reshape(1, H // P, W // P, -1).permute(0, 3, 1, 2)\n",
    "    gradients = gradients.reshape(1, H // P, W // P, -1).permute(0, 3, 1, 2)\n",
    "\n",
    "    # Compute Grad-CAM\n",
    "    pooled_gradients = torch.mean(gradients, dim=[2, 3], keepdim=True)\n",
    "    weighted_activation = pooled_gradients * activations\n",
    "    cam = torch.sum(weighted_activation, dim=1).squeeze()\n",
    "    cam = torch.relu(cam)\n",
    "\n",
    "    cam_np = cam.detach().cpu().numpy()\n",
    "    if np.max(cam_np) != 0:\n",
    "        cam_np = cam_np / np.max(cam_np)\n",
    "    else:\n",
    "        cam_np = np.zeros_like(cam_np)\n",
    "\n",
    "    return cam_np, target_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Function\n",
    "\n",
    "The main function puts everything together:\n",
    "\n",
    "1. It sets configuration values like the image path, number of classes, and model weights path.\n",
    "2. It loads the pre-trained model using `SwinTransformerClassifier.load_model`.\n",
    "3. It pre-processes the input image and sends it to the proper device.\n",
    "4. It computes the Grad-CAM, prints the predicted class, and generates the heatmap overlay.\n",
    "5. Finally, it displays the result using matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1002bde-76cb-4df2-a1e3-a071bd4db45c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward hook - Activations shape: torch.Size([1, 9216, 128])\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # ----------------------------\n",
    "    # Configuration / arguments\n",
    "    # ----------------------------\n",
    "    # Path to an image file (change to your image file path)\n",
    "    image_path = \"split/classification/F/test/Gl6/22_676_P1_3FG_F.png\"\n",
    "    \n",
    "    # Set the number of classes in your model\n",
    "    num_classes = 3\n",
    "    target_size = (384,384)\n",
    "    \n",
    "    # Path to your model weights\n",
    "    model_weight_path = \"Model_F_swintransformerclassifier_final_20250221_153017.pth\"\n",
    "    \n",
    "    # Load the model using the provided weights\n",
    "    model = SwinTransformerClassifier.load_model(\n",
    "                model_weight_path=model_weight_path,\n",
    "                num_classes=num_classes,\n",
    "                train_path=None,\n",
    "                val_path=None,\n",
    "                test_path=None,\n",
    "                optimizer=\"adam\",\n",
    "                lr=1e-3,\n",
    "                batch_size=16,\n",
    "                transfer=True,\n",
    "                tune_fc_only=True,\n",
    "                target_size=target_size\n",
    "            )\n",
    "    model.eval()\n",
    "\n",
    "    # Put model on device (CPU or GPU)\n",
    "    device = torch.device(\"cpu\")\n",
    "    # device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    # -------------------------------\n",
    "    # Preprocess the image\n",
    "    # -------------------------------\n",
    "    input_tensor, original_img = preprocess_image(image_path, target_size=target_size)\n",
    "    input_tensor = input_tensor.to(device)\n",
    "\n",
    "    # ----------------------------------\n",
    "    # Compute Grad-CAM\n",
    "    # ----------------------------------\n",
    "    cam_np, predicted_class = compute_gradcam(model, input_tensor, target_size)\n",
    "    print(\"Predicted class: \", predicted_class)\n",
    "\n",
    "    # Generate heatmap overlay on the original image\n",
    "    overlay = generate_heatmap_on_image(original_img, cam_np, alpha=0.5)\n",
    "\n",
    "    # Plot and display the result\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(overlay)\n",
    "    plt.axis('off')\n",
    "    plt.title(\"Grad-CAM Overlay\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Optionally, you can save the result\n",
    "    # plt.imsave(\"gradcam_overlay.png\", overlay)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook showed how to:\n",
    "\n",
    "- Use forward and backward hooks in PyTorch to capture intermediate activations and gradients.\n",
    "- Preprocess an input image using standard transforms.\n",
    "- Compute a Grad-CAM heatmap highlighting the regions in the image that contributed most to the prediction.\n",
    "- Overlay the heatmap on the original image and visualize the final result.\n",
    "\n",
    "Make sure to update the file paths (`image_path` and `model_weight_path`) to point to your own image and model weight files before running the notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "realmmlab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
